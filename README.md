# Hierarchical Transformers for Extractive Summarization


If you use this work, please cite the following references (accepted but pending publication): 

```
@article{DBLP:journals/jifs/GonzalezSGSH20,
  author    = {Jos{\'{e}}{-}{\'{A}}ngel Gonz{\'{a}}lez and
               Encarna Segarra and
               Fernando Garc{\'{\i}}a{-}Granada and
               Emilio Sanchis and
               Llu{\'{\i}}s{-}F. Hurtado},
  title     = {Extractive summarization using siamese hierarchical transformer encoders},
  journal   = {J. Intell. Fuzzy Syst.},
  volume    = {39},
  number    = {2},
  pages     = {2409--2419},
  year      = {2020},
  url       = {https://doi.org/10.3233/JIFS-179901},
  doi       = {10.3233/JIFS-179901},
  timestamp = {Thu, 10 Sep 2020 16:38:01 +0200},
  biburl    = {https://dblp.org/rec/journals/jifs/GonzalezSGSH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```


Hierarchical Transformers for Extractive Text Summarization

Sentence Encoder Layer 1 Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SentenceAttention-Layer1.PNG)

Sentence Encoder Layer 2 Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SentenceAttention-Layer2.PNG)

Sentence Encoder Layer 2 Avg Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/AvgHeadAttention-Layer2.png)


Sentence Encoder Layer 2 Avg Self Attentions Sum for Sentences (Final sentence relevance)
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SumSentenceAvgHeadAttention-Layer2.png)

