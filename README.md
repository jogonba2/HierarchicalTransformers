# Hierarchical Transformers for Extractive Summarization


If you use this work, please cite the following references (accepted, pending publication): 

```
 @article{shtelke19,
  added-at = {},
  author = {González, José-Ángel and Encarna, Segarra and García-Granada, Fernando and Sanchis, Emilio and Hurtado, Lluís-F.},
  biburl = {},
  ee = {},
  interhash = {},
  intrahash = {},
  journal = {Journal of Intelligent and Fuzzy Systems},
  number = {},
  pages = {},
  title = {Extractive Summarization using Siamese Hierarchical Transformer Encoders},
  url = {},
  volume = {},
  year = 2019
 }
```


Hierarchical Transformers for Extractive Text Summarization

Sentence Encoder Layer 1 Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SentenceAttention-Layer1.PNG)

Sentence Encoder Layer 2 Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SentenceAttention-Layer2.PNG)

Sentence Encoder Layer 2 Avg Self Attentions
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/AvgHeadAttention-Layer2.png)


Sentence Encoder Layer 2 Avg Self Attentions Sum for Sentences (Final sentence relevance)
![alt text](https://github.com/jogonba2/HierarchicalTransformers/blob/master/SumSentenceAvgHeadAttention-Layer2.png)


![alt text](https://i.gyazo.com/eb16336a42c2824efe5424dc01e72ab4.png)
